"""
æ™ºèƒ½å†…å®¹æå–å™¨ - é›†æˆå¤±è´¥æ¡ˆä¾‹åº“
================================
åŸºäºå†å²å¤±è´¥æ¡ˆä¾‹æ™ºèƒ½å†³ç­–æ˜¯å¦çˆ¬å–ï¼Œè‡ªåŠ¨ä½¿ç”¨SERPæ‘˜è¦ä½œä¸ºfallback
æ˜¾è‘—æå‡ç³»ç»Ÿæ•ˆç‡å’ŒæˆåŠŸç‡
"""

import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
import logging

from .content_extractor import ContentExtractor, ExtractionMethod

logger = logging.getLogger(__name__)


class SmartContentExtractor(ContentExtractor):
    """
    æ™ºèƒ½å†…å®¹æå–å™¨
    é›†æˆå¤±è´¥æ¡ˆä¾‹åº“ï¼Œè‡ªåŠ¨è·³è¿‡å·²çŸ¥å¤±è´¥çš„URLï¼Œä½¿ç”¨SERPæ‘˜è¦ä½œä¸ºå†…å®¹
    """
    
    def __init__(self, 
                 firecrawl_api_key: Optional[str] = None,
                 method: ExtractionMethod = ExtractionMethod.AUTO,
                 max_concurrent: int = 10,
                 timeout: int = 60,
                 enable_failure_learning: bool = True,
                 confidence_threshold: float = 0.7):
        """
        åˆå§‹åŒ–æ™ºèƒ½å†…å®¹æå–å™¨
        
        Args:
            enable_failure_learning: æ˜¯å¦å¯ç”¨å¤±è´¥æ¡ˆä¾‹å­¦ä¹ 
            confidence_threshold: è·³è¿‡çˆ¬å–çš„ç½®ä¿¡åº¦é˜ˆå€¼
        """
        super().__init__(firecrawl_api_key, method, max_concurrent, timeout)
        # NOTE: failure_learning åŠŸèƒ½å·²ç§»é™¤ï¼Œç®€åŒ–å®ç°
        self.enable_failure_learning = False  # å¼ºåˆ¶ç¦ç”¨
        self.confidence_threshold = confidence_threshold
        self.max_concurrent = max_concurrent

        logger.info(f"æ™ºèƒ½å†…å®¹æå–å™¨åˆå§‹åŒ–å®Œæˆ (failure_learningå·²ç¦ç”¨)")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await super().__aenter__()
        # NOTE: failure_db åˆå§‹åŒ–å·²ç§»é™¤
        return self
    
    async def smart_extract_content(self,
                                  url: str,
                                  serp_snippet: str = "",
                                  serp_title: str = "",
                                  fallback: bool = True,
                                  include_metadata: bool = True,
                                  force_crawl: bool = False) -> Dict[str, Any]:
        """
        æ™ºèƒ½å†…å®¹æå– - æ ¸å¿ƒæ–¹æ³•
        
        Args:
            url: è¦æå–çš„URL
            serp_snippet: SERPæœç´¢ç»“æœçš„æ‘˜è¦
            serp_title: SERPæœç´¢ç»“æœçš„æ ‡é¢˜
            fallback: æ˜¯å¦å¯ç”¨å¤±è´¥å›é€€
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®
            force_crawl: å¼ºåˆ¶çˆ¬å–ï¼Œå¿½ç•¥å¤±è´¥æ¡ˆä¾‹åº“
        
        Returns:
            æå–ç»“æœï¼ŒåŒ…å«æ™ºèƒ½å†³ç­–ä¿¡æ¯
        """
        start_time = datetime.now()
        
        # å‡†å¤‡SERP fallbackå†…å®¹
        serp_content = self._prepare_serp_content(serp_title, serp_snippet, url)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡çˆ¬å–
        should_skip = False
        skip_reason = ""
        confidence = 0.0
        
        if self.enable_failure_learning and self.failure_db and not force_crawl:
            should_skip, skip_reason, confidence = await self.failure_db.should_skip_crawl(url)
        
        if should_skip:
            # ä½¿ç”¨SERPæ‘˜è¦ä½œä¸ºå†…å®¹ï¼Œè·³è¿‡å®é™…çˆ¬å–
            logger.info(f"ğŸš€ æ™ºèƒ½è·³è¿‡çˆ¬å–: {skip_reason} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            result = {
                'success': True,
                'url': url,
                'content': serp_content,
                'method': 'serp_fallback',
                'is_serp_fallback': True,
                'skip_reason': skip_reason,
                'confidence': confidence,
                'content_length': len(serp_content),
                'extraction_time': (datetime.now() - start_time).total_seconds(),
                'metadata': {
                    'title': serp_title,
                    'snippet': serp_snippet,
                    'intelligent_skip': True
                } if include_metadata else {}
            }
            
            return result
        
        # æ­£å¸¸çˆ¬å–æµç¨‹
        logger.debug(f"ğŸ”„ æ‰§è¡Œæ­£å¸¸çˆ¬å–: {url}")
        
        try:
            # è°ƒç”¨çˆ¶ç±»çš„æå–æ–¹æ³•
            result = await super().extract_content(url, fallback, include_metadata)
            
            # è®°å½•æˆåŠŸæˆ–å¤±è´¥
            if self.enable_failure_learning and self.failure_db:
                if result['success']:
                    await self.failure_db.record_success(url)
                else:
                    # æå–é”™è¯¯ç±»å‹å’Œæ¶ˆæ¯
                    error_message = result.get('error', 'Unknown error')
                    failure_type = self._classify_failure_type(error_message, result)
                    await self.failure_db.record_failure(url, failure_type, error_message)
            
            # å¦‚æœçˆ¬å–å¤±è´¥ä¸”æœ‰SERPå†…å®¹ï¼Œä½¿ç”¨SERPä½œä¸ºfallback
            if not result['success'] and serp_content and fallback:
                logger.info(f"ğŸ’¡ çˆ¬å–å¤±è´¥ï¼Œä½¿ç”¨SERPæ‘˜è¦ä½œä¸ºfallback: {url}")
                
                result = {
                    'success': True,
                    'url': url,
                    'content': serp_content,
                    'method': 'serp_fallback_after_failure',
                    'is_serp_fallback': True,
                    'original_error': result.get('error', ''),
                    'content_length': len(serp_content),
                    'extraction_time': (datetime.now() - start_time).total_seconds(),
                    'metadata': {
                        'title': serp_title,
                        'snippet': serp_snippet,
                        'fallback_after_failure': True
                    } if include_metadata else {}
                }
            
            # æ·»åŠ æ™ºèƒ½æå–æ ‡è®°
            result['intelligent_extraction'] = True
            result['failure_learning_enabled'] = self.enable_failure_learning
            
            return result
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½å†…å®¹æå–å¼‚å¸¸ {url}: {str(e)}")
            
            # è®°å½•å¼‚å¸¸
            if self.enable_failure_learning and self.failure_db:
                await self.failure_db.record_failure(url, 'extraction_exception', str(e))
            
            # å¦‚æœæœ‰SERPå†…å®¹ï¼Œä½¿ç”¨ä½œä¸ºfallback
            if serp_content and fallback:
                logger.info(f"ğŸ’¡ æå–å¼‚å¸¸ï¼Œä½¿ç”¨SERPæ‘˜è¦ä½œä¸ºfallback: {url}")
                return {
                    'success': True,
                    'url': url,
                    'content': serp_content,
                    'method': 'serp_fallback_after_exception',
                    'is_serp_fallback': True,
                    'original_exception': str(e),
                    'content_length': len(serp_content),
                    'extraction_time': (datetime.now() - start_time).total_seconds(),
                    'metadata': {
                        'title': serp_title,
                        'snippet': serp_snippet,
                        'fallback_after_exception': True
                    } if include_metadata else {}
                }
            
            # å®Œå…¨å¤±è´¥
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'content': None,
                'extraction_time': (datetime.now() - start_time).total_seconds()
            }
    
    def _prepare_serp_content(self, title: str, snippet: str, url: str) -> str:
        """å‡†å¤‡SERPå†…å®¹ä½œä¸ºfallback"""
        if not title and not snippet:
            return ""
        
        content_parts = []
        
        if title:
            content_parts.append(f"# {title}")
        
        if snippet:
            # æ¸…ç†å’Œæ ¼å¼åŒ–snippet
            cleaned_snippet = snippet.strip()
            if cleaned_snippet:
                content_parts.append(f"\n{cleaned_snippet}")
        
        if url:
            content_parts.append(f"\n\næ¥æº: {url}")
        
        content_parts.append(f"\n\n*æ³¨ï¼šæ­¤å†…å®¹æ¥æºäºæœç´¢ç»“æœæ‘˜è¦*")
        
        return "\n".join(content_parts)
    
    def _classify_failure_type(self, error_message: str, result: Dict) -> str:
        """åˆ†ç±»å¤±è´¥ç±»å‹"""
        error_lower = error_message.lower()
        
        if '403' in error_message or 'forbidden' in error_lower:
            return 'HTTP_403'
        elif '404' in error_message or 'not found' in error_lower:
            return 'HTTP_404'
        elif '429' in error_message or 'rate limit' in error_lower:
            return 'RATE_LIMITED'
        elif 'timeout' in error_lower:
            return 'TIMEOUT'
        elif 'ssl' in error_lower or 'certificate' in error_lower:
            return 'SSL_ERROR'
        elif 'dns' in error_lower:
            return 'DNS_ERROR'
        elif 'parse' in error_lower or 'extract' in error_lower:
            return 'PARSE_ERROR'
        elif 'connect' in error_lower:
            return 'CONNECTION_ERROR'
        else:
            return 'OTHER'
    
    async def batch_smart_extract(self,
                                 url_data: List[Dict],
                                 fallback: bool = True,
                                 include_metadata: bool = True) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æ™ºèƒ½å†…å®¹æå–
        
        Args:
            url_data: URLæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åº”åŒ…å«:
                - url: è¦æå–çš„URL
                - title: SERPæ ‡é¢˜ (å¯é€‰)
                - snippet: SERPæ‘˜è¦ (å¯é€‰)
        
        Returns:
            æå–ç»“æœåˆ—è¡¨
        """
        tasks = []
        
        for item in url_data:
            url = item.get('url', '')
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            
            if url:
                task = self.smart_extract_content(
                    url=url,
                    serp_snippet=snippet,
                    serp_title=title,
                    fallback=fallback,
                    include_metadata=include_metadata
                )
                tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æå–å¼‚å¸¸ [{i}]: {str(result)}")
                processed_results.append({
                    'success': False,
                    'url': url_data[i].get('url', ''),
                    'error': str(result),
                    'content': None
                })
            else:
                processed_results.append(result)
        
        # ç»Ÿè®¡ä¿¡æ¯
        success_count = sum(1 for r in processed_results if r.get('success'))
        serp_fallback_count = sum(1 for r in processed_results if r.get('is_serp_fallback'))
        
        logger.info(f"æ‰¹é‡æ™ºèƒ½æå–å®Œæˆ: {success_count}/{len(url_data)} æˆåŠŸ, {serp_fallback_count} ä¸ªä½¿ç”¨SERP fallback")
        
        return processed_results
    
    async def get_learning_stats(self) -> str:
        """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_failure_learning or not self.failure_db:
            return "å¤±è´¥æ¡ˆä¾‹å­¦ä¹ æœªå¯ç”¨"
        
        return await self.failure_db.get_stats_report()
    
    async def force_learn_failure(self, url: str, failure_type: str = "MANUAL", 
                                 error_message: str = "æ‰‹åŠ¨æ ‡è®°ä¸ºå¤±è´¥"):
        """æ‰‹åŠ¨æ ‡è®°URLä¸ºå¤±è´¥æ¡ˆä¾‹"""
        if self.enable_failure_learning and self.failure_db:
            await self.failure_db.record_failure(url, failure_type, error_message)
            logger.info(f"æ‰‹åŠ¨æ ‡è®°å¤±è´¥æ¡ˆä¾‹: {url}")
    
    async def cleanup_old_failures(self, days: int = 30):
        """æ¸…ç†æ—§çš„å¤±è´¥è®°å½•"""
        if self.enable_failure_learning and self.failure_db:
            await self.failure_db.cleanup_old_records(days)


# ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    """æ™ºèƒ½å†…å®¹æå–å™¨ä½¿ç”¨ç¤ºä¾‹"""
    
    async with SmartContentExtractor(enable_failure_learning=True) as extractor:
        
        # å•ä¸ªURLæ™ºèƒ½æå–
        result = await extractor.smart_extract_content(
            url="https://finance.yahoo.com/quote/TSLA/history/",
            serp_snippet="Tesla stock price history and chart data",
            serp_title="Tesla, Inc. (TSLA) Stock Price History"
        )
        
        if result['success']:
            print(f"æå–æˆåŠŸ: æ–¹æ³•={result['method']}, é•¿åº¦={result['content_length']}")
            if result.get('is_serp_fallback'):
                print("ä½¿ç”¨äº†SERPæ‘˜è¦ä½œä¸ºå†…å®¹")
        
        # æ‰¹é‡æ™ºèƒ½æå–
        url_data = [
            {
                'url': 'https://www.nasdaq.com/market-activity/stocks/tsla',
                'title': 'TSLA Stock Price',
                'snippet': 'Tesla stock price and market data'
            },
            {
                'url': 'https://finance.yahoo.com/quote/TSLA/',
                'title': 'Tesla Stock',
                'snippet': 'Real-time Tesla stock price and analysis'
            }
        ]
        
        batch_results = await extractor.batch_smart_extract(url_data)
        
        print(f"æ‰¹é‡æå–å®Œæˆ: {len(batch_results)} ä¸ªç»“æœ")
        for i, result in enumerate(batch_results):
            print(f"  {i+1}. {result['url'][:50]}... -> {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
        
        # è·å–å­¦ä¹ ç»Ÿè®¡
        stats = await extractor.get_learning_stats()
        print(f"\nå­¦ä¹ ç»Ÿè®¡:\n{stats}")


if __name__ == "__main__":
    asyncio.run(example_usage())