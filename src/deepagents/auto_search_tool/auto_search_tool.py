"""
Automated Search Tool
====================
Combines web_search â†’ auto fetch â†’ PDF parsing into a single workflow
No LLM intervention needed for tool coordination
"""

import asyncio
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime
import os

from .brightdata_client import BrightDataAsyncClient
from ..utils.content_extractor import ContentExtractor, ExtractionMethod
from ..utils.smart_content_extractor import SmartContentExtractor
from ..utils.pdf_parser import PDFParser

logger = logging.getLogger(__name__)

# æ ¹æ®ç¯å¢ƒå˜é‡è®¾ç½®æ—¥å¿—çº§åˆ«
search_debug_enabled = os.getenv('SEARCH_DEBUG_LOG', 'false').lower() == 'true'
if search_debug_enabled:
    logger.setLevel(logging.DEBUG)
    # åŒæ—¶è®¾ç½® BrightData å®¢æˆ·ç«¯çš„æ—¥å¿—çº§åˆ«
    brightdata_logger = logging.getLogger('src.tools.brightdata_client')
    brightdata_logger.setLevel(logging.DEBUG)
    # è®¾ç½®å†…å®¹æå–å™¨çš„æ—¥å¿—çº§åˆ«
    extractor_logger = logging.getLogger('src.utils.content_extractor')
    extractor_logger.setLevel(logging.DEBUG)
    smart_extractor_logger = logging.getLogger('src.utils.smart_content_extractor')
    smart_extractor_logger.setLevel(logging.DEBUG)
    logger.info("ğŸ” æœç´¢è°ƒè¯•æ—¥å¿—å·²å¼€å¯ (SEARCH_DEBUG_LOG=true)")


class AutoSearchTool:
    """
    è‡ªåŠ¨åŒ–æœç´¢å·¥å…·
    è‡ªåŠ¨æ‰§è¡Œï¼šæœç´¢ â†’ æŠ“å–æ‰€æœ‰ç»“æœ â†’ PDFè‡ªåŠ¨è§£æ
    """
    
    def __init__(self,
                 brightdata_api_key: str,
                 firecrawl_api_key: Optional[str] = None,
                 max_concurrent_fetch: int = 3,
                 auto_fetch_limit: int = 3,
                 max_content_length: int = 10000,
                 max_content_tokens: int = 3000,
                 enable_smart_extraction: bool = True,
                 confidence_threshold: float = 0.7,
                 parallel_timeout: float = 30.0,
                 single_url_timeout: float = 15.0):
        """
        åˆå§‹åŒ–è‡ªåŠ¨æœç´¢å·¥å…·
        
        Args:
            brightdata_api_key: BrightData APIå¯†é’¥
            firecrawl_api_key: FireCrawl APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            max_concurrent_fetch: æœ€å¤§å¹¶å‘æŠ“å–æ•°
            auto_fetch_limit: è‡ªåŠ¨æŠ“å–çš„æœ€å¤§ç»“æœæ•°
            max_content_length: æ¯ä¸ªå†…å®¹çš„æœ€å¤§é•¿åº¦
            max_content_tokens: æ¯ä¸ªå†…å®¹çš„æœ€å¤§tokenæ•°ï¼ˆä½¿ç”¨å­—ç¬¦æ•°/3.5ä¼°ç®—ï¼‰
            enable_smart_extraction: æ˜¯å¦å¯ç”¨æ™ºèƒ½æå–ï¼ˆå¤±è´¥å­¦ä¹ æœºåˆ¶ï¼‰
            confidence_threshold: è·³è¿‡çˆ¬å–çš„ç½®ä¿¡åº¦é˜ˆå€¼
            parallel_timeout: å¹¶è¡ŒURLæŠ“å–çš„æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            single_url_timeout: å•ä¸ªURLçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.brightdata_client = BrightDataAsyncClient(
            api_key=brightdata_api_key,
            max_concurrent=3
        )
        
        # æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨æ™ºèƒ½æå–å™¨æˆ–æ™®é€šæå–å™¨
        if enable_smart_extraction:
            self.content_extractor = SmartContentExtractor(
                firecrawl_api_key=firecrawl_api_key,
                method=ExtractionMethod.AUTO,
                max_concurrent=max_concurrent_fetch,
                enable_failure_learning=True,
                confidence_threshold=confidence_threshold
            )
            logger.info(f"å¯ç”¨æ™ºèƒ½å†…å®¹æå–å™¨ï¼Œç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
        else:
            self.content_extractor = ContentExtractor(
                firecrawl_api_key=firecrawl_api_key,
                method=ExtractionMethod.AUTO,
                max_concurrent=max_concurrent_fetch
            )
            logger.info("ä½¿ç”¨æ™®é€šå†…å®¹æå–å™¨")
        
        self.enable_smart_extraction = enable_smart_extraction
        
        self.auto_fetch_limit = auto_fetch_limit
        self.max_content_length = max_content_length
        self.max_content_tokens = max_content_tokens
        # å­—ç¬¦åˆ°tokençš„æ¢ç®—æ¯”ä¾‹ï¼ˆ1 token â‰ˆ 4 å­—ç¬¦ï¼Œä¿å®ˆä¼°è®¡ï¼‰
        self.char_to_token_ratio = 4.0
        self.parallel_timeout = parallel_timeout
        self.single_url_timeout = single_url_timeout
        
        logger.info(f"è‡ªåŠ¨æœç´¢å·¥å…·åˆå§‹åŒ–å®Œæˆï¼Œè‡ªåŠ¨æŠ“å–é™åˆ¶: {auto_fetch_limit}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.brightdata_client.__aenter__()
        await self.content_extractor.__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.brightdata_client.__aexit__(exc_type, exc_val, exc_tb)
        # KISSæ–¹æ¡ˆï¼šcontent_extractorç°åœ¨ç®¡ç†è‡ªå·±çš„sessionç”Ÿå‘½å‘¨æœŸ
        await self.content_extractor.__aexit__(exc_type, exc_val, exc_tb)
    
    async def search_and_fetch(self,
                             query: str,
                             num_results: int = 10,
                             mode: str = "full",
                             search_options: Optional[Dict] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œæœç´¢å¹¶æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦æŠ“å–å†…å®¹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            num_results: æœç´¢ç»“æœæ•°é‡
            mode: æœç´¢æ¨¡å¼
                - "light": åªè¿”å›æœç´¢ç»“æœå’Œsnippetsï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                - "full": è‡ªåŠ¨æŠ“å–å®Œæ•´å†…å®¹å¹¶è§£æPDFï¼ˆæ·±åº¦æ¨¡å¼ï¼‰
            search_options: æœç´¢é€‰é¡¹ï¼ˆä¼ é€’ç»™BrightDataï¼‰
        
        Returns:
            åŒ…å«æœç´¢ç»“æœå’ŒæŠ“å–å†…å®¹çš„å­—å…¸
        """
        start_time = datetime.now()
        search_options = search_options or {}
        
        # åˆ›å»ºè¯·æ±‚IDç”¨äºè¿½è¸ª
        request_id = f"search_{datetime.now().strftime('%H%M%S')}_{query[:20]}"
        if search_debug_enabled:
            logger.info(f"ğŸ” [REQ-{request_id}] å¼€å§‹æœç´¢: {query} | mode: {mode} | num_results: {num_results}")
        else:
            logger.info(f"ğŸ” å¼€å§‹æœç´¢: {query}")
        
        # Step 1: æ‰§è¡Œæœç´¢
        search_start = datetime.now()
        search_result = await self.brightdata_client.search(
            query=query,
            num_results=num_results,
            **search_options
        )
        search_elapsed = (datetime.now() - search_start).total_seconds()
        if search_debug_enabled:
            logger.info(f"â±ï¸ [REQ-{request_id}] BrightDataæœç´¢è€—æ—¶: {search_elapsed:.2f}ç§’")
        
        if not search_result['success']:
            logger.error(f"âŒ [REQ-{request_id}] æœç´¢å¤±è´¥: {search_result.get('error', 'Unknown error')}")
            return {
                'success': False,
                'query': query,
                'error': search_result.get('error', 'Search failed'),
                'stage': 'search',
                'request_id': request_id,
                'elapsed': search_elapsed
            }
        
        search_results = search_result.get('results', [])
        logger.info(f"âœ… [REQ-{request_id}] æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
        
        # Light mode: ç›´æ¥è¿”å›æœç´¢ç»“æœï¼Œä¸æŠ“å–å†…å®¹
        if mode == "light":
            logger.info("ğŸ’¡ Lightæ¨¡å¼ï¼šä»…è¿”å›æœç´¢ç»“æœå’Œsnippets")
            enhanced_results = []
            for i, result in enumerate(search_results):
                enhanced_results.append({
                    'url': result.get('url', ''),
                    'title': result.get('title', ''),
                    'snippet': result.get('snippet', ''),
                    'position': result.get('position', i + 1),
                    'fetch_success': False,
                    'fetch_reason': 'light_mode',
                    'content': None
                })
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            return {
                'success': True,
                'query': query,
                'mode': 'light',
                'results': enhanced_results,
                'statistics': {
                    'total_results': len(search_results),
                    'auto_fetched': 0,
                    'fetch_success': 0,
                    'pdf_count': 0,
                    'elapsed': elapsed
                }
            }
        
        # Full mode: è‡ªåŠ¨æŠ“å–å‰Nä¸ªç»“æœ
        logger.info("ğŸ” Fullæ¨¡å¼ï¼šè‡ªåŠ¨æŠ“å–å®Œæ•´å†…å®¹")
        urls_to_fetch = []
        for i, result in enumerate(search_results[:self.auto_fetch_limit]):
            if result.get('url'):
                urls_to_fetch.append({
                    'url': result['url'],
                    'title': result.get('title', ''),
                    'snippet': result.get('snippet', ''),
                    'position': result.get('position', i + 1)
                })
        
        if search_debug_enabled:
            logger.info(f"ğŸ“¥ [REQ-{request_id}] å¼€å§‹å¹¶è¡ŒæŠ“å– {len(urls_to_fetch)} ä¸ªURL")
            for idx, item in enumerate(urls_to_fetch):
                logger.debug(f"  [{idx+1}] {item['url']}")
        else:
            logger.info(f"ğŸ“¥ å¼€å§‹è‡ªåŠ¨æŠ“å– {len(urls_to_fetch)} ä¸ªURL")
        
        # Normal/Deepæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°
        extractor_max_concurrent = getattr(self.content_extractor, "max_concurrent", 3)
        effective_concurrent = min(extractor_max_concurrent, len(urls_to_fetch))
        
        if search_debug_enabled:
            logger.debug(f"ğŸ”§ [REQ-{request_id}] å¹¶å‘ç­–ç•¥: mode={mode}, concurrent={effective_concurrent}")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(effective_concurrent)
        
        async def fetch_with_semaphore(item):
            async with semaphore:
                return await self._fetch_and_process(item)
        
        # å¹¶å‘æŠ“å–æ‰€æœ‰URLï¼ˆå¸¦æ€»ä½“è¶…æ—¶æ§åˆ¶ï¼‰
        fetch_tasks = []
        for idx, item in enumerate(urls_to_fetch):
            # ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ ç´¢å¼•å’Œè¯·æ±‚ID
            item['_index'] = idx
            item['_request_id'] = request_id
            task = fetch_with_semaphore(item)
            fetch_tasks.append(task)
        
        # æ·»åŠ æ€»ä½“è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢å¹¶è¡ŒæŠ“å–è€—æ—¶è¿‡é•¿
        fetch_start = datetime.now()
        try:
            effective_timeout = self.parallel_timeout

            if search_debug_enabled:
                logger.debug(f"ğŸ•’ [REQ-{request_id}] å¹¶è¡ŒæŠ“å–è¶…æ—¶è®¾ç½®: {effective_timeout}ç§’")
            
            fetch_results = await asyncio.wait_for(
                asyncio.gather(*fetch_tasks, return_exceptions=True),
                timeout=effective_timeout
            )
            fetch_elapsed = (datetime.now() - fetch_start).total_seconds()
            logger.info(f"â±ï¸ [REQ-{request_id}] å¹¶è¡ŒæŠ“å–æ€»è€—æ—¶: {fetch_elapsed:.2f}ç§’")
        except asyncio.TimeoutError:
            fetch_elapsed = (datetime.now() - fetch_start).total_seconds()
            logger.warning(f"âš ï¸ [REQ-{request_id}] URLæ‰¹é‡æŠ“å–è¶…æ—¶ï¼ˆ{fetch_elapsed:.2f}ç§’ï¼‰ï¼Œå–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡")
            logger.debug(f"âš ï¸ [REQ-{request_id}] è¶…æ—¶ç±»å‹: URLå¹¶è¡ŒæŠ“å–çº§åˆ« | é™åˆ¶: {effective_timeout}ç§’ | æ¨¡å¼: {mode}")
            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            for task in fetch_tasks:
                if not task.done():
                    task.cancel()
            # æ”¶é›†å·²å®Œæˆçš„ç»“æœ
            fetch_results = []
            for task in fetch_tasks:
                try:
                    if task.done() and not task.cancelled():
                        fetch_results.append(task.result())
                    else:
                        fetch_results.append(Exception("Task cancelled due to timeout"))
                except Exception as e:
                    fetch_results.append(e)
        
        # æ•´åˆç»“æœ
        enhanced_results = []
        fetch_success_count = 0
        pdf_count = 0
        
        for i, (item, fetch_result) in enumerate(zip(urls_to_fetch, fetch_results)):
            if isinstance(fetch_result, Exception):
                logger.error(f"æŠ“å–å¼‚å¸¸ {item['url']}: {str(fetch_result)}")
                enhanced_result = {
                    **item,
                    'fetch_success': False,
                    'fetch_error': str(fetch_result),
                    'content': None
                }
            else:
                enhanced_result = {
                    **item,
                    **fetch_result
                }
                if fetch_result.get('fetch_success'):
                    fetch_success_count += 1
                if fetch_result.get('is_pdf'):
                    pdf_count += 1
            
            # KISS: å¦‚æœæ²¡æœ‰ content ä½†æœ‰ snippetï¼Œç”¨ snippet ä½œä¸º content
            if enhanced_result.get('content') is None and enhanced_result.get('snippet'):
                enhanced_result['content'] = enhanced_result['snippet']
                enhanced_result['extraction_method'] = enhanced_result.get('extraction_method', 'snippet_fallback')
            
            enhanced_results.append(enhanced_result)
        
        # æ·»åŠ æœªæŠ“å–çš„æœç´¢ç»“æœï¼ˆä¿ç•™snippetï¼‰
        for result in search_results[self.auto_fetch_limit:]:
            snippet = result.get('snippet', '')
            enhanced_results.append({
                'url': result.get('url', ''),
                'title': result.get('title', ''),
                'snippet': snippet,
                'position': result.get('position', 0),
                'fetch_success': False,
                'fetch_reason': 'exceeded_auto_fetch_limit',
                'content': snippet,  # KISS: ç›´æ¥ä½¿ç”¨ snippet ä½œä¸º content
                'extraction_method': 'snippet_only'
            })
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        # è®°å½•è¯¦ç»†ç»Ÿè®¡
        total_elapsed = (datetime.now() - start_time).total_seconds()
        
        if search_debug_enabled:
            logger.info(f"âœ… [REQ-{request_id}] è‡ªåŠ¨æœç´¢å®Œæˆ:")
            logger.info(f"   - æ€»è€—æ—¶: {total_elapsed:.2f}ç§’ (æœç´¢: {search_elapsed:.2f}ç§’)")
            logger.info(f"   - æˆåŠŸæŠ“å–: {fetch_success_count}/{len(urls_to_fetch)}")
            logger.info(f"   - PDFæ–‡æ¡£: {pdf_count}")
            
            # è®°å½•å¤±è´¥çš„URL
            failed_urls = [r for r in enhanced_results[:len(urls_to_fetch)] if not r.get('fetch_success', False)]
            if failed_urls:
                logger.warning(f"âŒ [REQ-{request_id}] å¤±è´¥çš„URL ({len(failed_urls)}ä¸ª):")
                for failed in failed_urls:
                    logger.warning(f"   - {failed.get('url', 'Unknown')}: {failed.get('fetch_error', 'Unknown error')}")
        else:
            logger.info(f"âœ… è‡ªåŠ¨æœç´¢å®Œæˆ: {fetch_success_count}/{len(urls_to_fetch)} æˆåŠŸæŠ“å–, {pdf_count} ä¸ªPDF")
        
        # æ€§èƒ½åˆ†ææ—¥å¿—
        if logger.isEnabledFor(logging.DEBUG):
            # è®¡ç®—æ¯ä¸ªURLçš„å¹³å‡è€—æ—¶
            url_timings = [r.get('elapsed', 0) for r in enhanced_results[:len(urls_to_fetch)] if 'elapsed' in r]
            if url_timings:
                avg_time = sum(url_timings) / len(url_timings)
                max_time = max(url_timings)
                min_time = min(url_timings)
                logger.debug(f"ğŸ“Š [REQ-{request_id}] URLæŠ“å–æ€§èƒ½åˆ†æ:")
                logger.debug(f"   - å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’")
                logger.debug(f"   - æœ€å¿«: {min_time:.2f}ç§’")
                logger.debug(f"   - æœ€æ…¢: {max_time:.2f}ç§’")
        
        return {
            'success': True,
            'query': query,
            'request_id': request_id,
            'results': enhanced_results,
            'statistics': {
                'total_results': len(search_results),
                'auto_fetched': len(urls_to_fetch),
                'fetch_success': fetch_success_count,
                'pdf_count': pdf_count,
                'elapsed': total_elapsed,
                'search_elapsed': search_elapsed,
                'fetch_elapsed': fetch_elapsed if 'fetch_elapsed' in locals() else 0
            }
        }
    
    async def _fetch_and_process(self, item: Dict) -> Dict[str, Any]:
        """
        æŠ“å–å¹¶å¤„ç†å•ä¸ªURL
        
        Args:
            item: åŒ…å«url, title, snippetç­‰ä¿¡æ¯çš„å­—å…¸
        
        Returns:
            å¤„ç†ç»“æœ
        """
        url = item['url']
        title = item.get('title', '')
        snippet = item.get('snippet', '')
        idx = item.get('_index', -1)
        request_id = item.get('_request_id', 'unknown')
        
        fetch_start = datetime.now()
        if search_debug_enabled:
            logger.debug(f"ğŸ”— [REQ-{request_id}][URL-{idx+1}] å¼€å§‹æŠ“å–: {url[:80]}...")
        
        try:
            # ä¸ºå•ä¸ªå†…å®¹æå–æ·»åŠ è¶…æ—¶æ§åˆ¶
            single_fetch_timeout = self.single_url_timeout  # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
            
            # ä½¿ç”¨æ™ºèƒ½å†…å®¹æå–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–æ™®é€šæå–å™¨
            if self.enable_smart_extraction and hasattr(self.content_extractor, 'smart_extract_content'):
                # ä½¿ç”¨æ™ºèƒ½æå–ï¼Œä¼ é€’SERPæ•°æ®ä½œä¸ºfallback
                extract_result = await asyncio.wait_for(
                    self.content_extractor.smart_extract_content(
                        url=url,
                        serp_snippet=snippet,
                        serp_title=title,
                        fallback=True,
                        include_metadata=True
                    ),
                    timeout=single_fetch_timeout
                )
            else:
                # ä½¿ç”¨æ™®é€šæå–å™¨
                extract_result = await asyncio.wait_for(
                    self.content_extractor.extract_content(
                        url=url,
                        fallback=True,
                        include_metadata=True
                    ),
                    timeout=single_fetch_timeout
                )
            
            if extract_result['success']:
                content = extract_result.get('content', '')
                fetch_elapsed = (datetime.now() - fetch_start).total_seconds()
                
                # ç®€å•tokenä¼°ç®—ï¼šå­—ç¬¦æ•° / 3.5
                estimated_tokens = len(content) / self.char_to_token_ratio
                
                # å…ˆæ£€æŸ¥tokené™åˆ¶ï¼Œå†æ£€æŸ¥å­—ç¬¦é™åˆ¶
                if estimated_tokens > self.max_content_tokens:
                    # æ ¹æ®tokené™åˆ¶è®¡ç®—å…è®¸çš„æœ€å¤§å­—ç¬¦æ•°
                    max_chars = int(self.max_content_tokens * self.char_to_token_ratio)
                    content = content[:max_chars] + '\n\n[å†…å®¹å·²æˆªæ–­]'
                    is_truncated = True
                    estimated_tokens = self.max_content_tokens
                elif len(content) > self.max_content_length:
                    content = content[:self.max_content_length] + '\n\n[å†…å®¹å·²æˆªæ–­]'
                    is_truncated = True
                    estimated_tokens = len(content) / self.char_to_token_ratio
                else:
                    is_truncated = extract_result.get('is_truncated', False)
                
                if search_debug_enabled:
                    logger.info(f"âœ… [REQ-{request_id}][URL-{idx+1}] æˆåŠŸæŠ“å– | è€—æ—¶: {fetch_elapsed:.2f}ç§’ | å†…å®¹: {len(content)}å­—ç¬¦ | Tokens: {int(estimated_tokens)}")
                
                result_dict = {
                    'fetch_success': True,
                    'content': content,
                    'content_length': len(content),
                    'estimated_tokens': int(estimated_tokens),
                    'is_truncated': is_truncated,
                    'extraction_method': extract_result.get('method', 'unknown'),
                    'is_pdf': extract_result.get('method') == 'pdf_parser',
                    'metadata': extract_result.get('metadata', {})
                }
                
                # æ·»åŠ æ™ºèƒ½æå–ç›¸å…³ä¿¡æ¯
                if extract_result.get('is_serp_fallback'):
                    result_dict['is_serp_fallback'] = True
                    result_dict['skip_reason'] = extract_result.get('skip_reason', '')
                    result_dict['confidence'] = extract_result.get('confidence', 0.0)
                    logger.info(f"ğŸš€ ä½¿ç”¨SERP fallback: {url} - {result_dict['skip_reason']}")
                
                if extract_result.get('intelligent_extraction'):
                    result_dict['intelligent_extraction'] = True
                    result_dict['failure_learning_enabled'] = extract_result.get('failure_learning_enabled', False)
                
                return result_dict
            else:
                fetch_elapsed = (datetime.now() - fetch_start).total_seconds()
                error_msg = extract_result.get('error', 'Unknown error')
                if search_debug_enabled:
                    logger.warning(f"âŒ [REQ-{request_id}][URL-{idx+1}] æŠ“å–å¤±è´¥ | è€—æ—¶: {fetch_elapsed:.2f}ç§’ | é”™è¯¯: {error_msg}")
                return {
                    'fetch_success': False,
                    'fetch_error': error_msg,
                    'content': None,
                    'elapsed': fetch_elapsed
                }
                
        except asyncio.TimeoutError:
            fetch_elapsed = (datetime.now() - fetch_start).total_seconds()
            if search_debug_enabled:
                logger.warning(f"â±ï¸ [REQ-{request_id}][URL-{idx+1}] å•URLå†…å®¹æå–è¶…æ—¶ | è€—æ—¶: {fetch_elapsed:.2f}ç§’ (é™åˆ¶: {single_fetch_timeout}ç§’) | URL: {url[:80]}")
                logger.debug(f"â±ï¸ [REQ-{request_id}][URL-{idx+1}] è¶…æ—¶ç±»å‹: å•ä¸ªURLå†…å®¹æå–çº§åˆ«")
            else:
                logger.warning(f"â±ï¸ å†…å®¹æå–è¶…æ—¶: {url[:80]}")
            return {
                'fetch_success': False,
                'fetch_error': f'Content extraction timeout ({single_fetch_timeout}s)',
                'content': snippet,  # ä½¿ç”¨snippetä½œä¸ºfallback
                'is_timeout': True,
                'elapsed': fetch_elapsed
            }
        except Exception as e:
            fetch_elapsed = (datetime.now() - fetch_start).total_seconds()
            if search_debug_enabled:
                logger.error(f"âŒ [REQ-{request_id}][URL-{idx+1}] å¤„ç†å¤±è´¥ | è€—æ—¶: {fetch_elapsed:.2f}ç§’ | é”™è¯¯: {str(e)} | URL: {url[:80]}")
            else:
                logger.error(f"âŒ å¤„ç†URLå¤±è´¥: {url[:80]} - {str(e)}")
            return {
                'fetch_success': False,
                'fetch_error': str(e),
                'content': None,
                'elapsed': fetch_elapsed
            }
    
    async def get_smart_extraction_stats(self) -> str:
        """è·å–æ™ºèƒ½æå–ç»Ÿè®¡ä¿¡æ¯"""
        if self.enable_smart_extraction and hasattr(self.content_extractor, 'get_learning_stats'):
            return await self.content_extractor.get_learning_stats()
        else:
            return "æ™ºèƒ½æå–æœªå¯ç”¨"
    
    async def force_learn_failure(self, url: str, failure_type: str = "MANUAL", 
                                 error_message: str = "æ‰‹åŠ¨æ ‡è®°ä¸ºå¤±è´¥"):
        """æ‰‹åŠ¨æ ‡è®°URLä¸ºå¤±è´¥æ¡ˆä¾‹"""
        if self.enable_smart_extraction and hasattr(self.content_extractor, 'force_learn_failure'):
            await self.content_extractor.force_learn_failure(url, failure_type, error_message)
        else:
            logger.warning("æ™ºèƒ½æå–æœªå¯ç”¨ï¼Œæ— æ³•æ ‡è®°å¤±è´¥æ¡ˆä¾‹")


def create_auto_search_tool(brightdata_api_key: str,
                          firecrawl_api_key: Optional[str] = None,
                          auto_fetch_limit: int = 5,
                          enable_smart_extraction: bool = True,
                          confidence_threshold: float = 0.7) -> Dict:
    """
    åˆ›å»ºè‡ªåŠ¨æœç´¢å·¥å…·ï¼ˆLangChainæ ¼å¼ï¼‰
    
    Args:
        brightdata_api_key: BrightData APIå¯†é’¥
        firecrawl_api_key: FireCrawl APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
        auto_fetch_limit: è‡ªåŠ¨æŠ“å–çš„æœ€å¤§URLæ•°é‡ï¼Œé»˜è®¤5ä¸ª
        enable_smart_extraction: æ˜¯å¦å¯ç”¨æ™ºèƒ½æå–ï¼ˆå¤±è´¥å­¦ä¹ æœºåˆ¶ï¼‰
        confidence_threshold: è·³è¿‡çˆ¬å–çš„ç½®ä¿¡åº¦é˜ˆå€¼
    
    Returns:
        LangChainå·¥å…·å®šä¹‰
    """
    from langchain_core.tools import StructuredTool
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    tool_instance = AutoSearchTool(
        brightdata_api_key=brightdata_api_key,
        firecrawl_api_key=firecrawl_api_key,
        auto_fetch_limit=auto_fetch_limit,
        enable_smart_extraction=enable_smart_extraction,
        confidence_threshold=confidence_threshold
    )
    
    async def auto_search(query: str, num_results: int = 10, mode: str = "full", **kwargs) -> Dict[str, Any]:
        """
        æ™ºèƒ½æœç´¢å·¥å…·ï¼Œæ”¯æŒå•ä¸€æœç´¢å’Œå¹¶è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼:
                - å•ä¸€æœç´¢: "<search>OpenAI GPT-4</search>"
                - å¹¶è¡Œæœç´¢: "<search>OpenAI GPT-4|Google Gemini|Claude 3</search>"
            num_results: æœç´¢ç»“æœæ•°é‡ï¼ˆå¹¶è¡Œæœç´¢æ—¶ä¼šå¹³åˆ†ç»™å„æŸ¥è¯¢ï¼‰
            mode: æœç´¢æ¨¡å¼
                - "light": åªè¿”å›æœç´¢ç»“æœå’Œsnippetsï¼Œä¸æŠ“å–å†…å®¹
                - "full": è‡ªåŠ¨æŠ“å–å†…å®¹å¹¶è§£æPDF
        
        Returns:
            æœç´¢å’ŒæŠ“å–ç»“æœï¼ŒåŒ…å«å¹¶è¡Œæœç´¢çš„ç»Ÿè®¡ä¿¡æ¯
        """
        # KISS: åªå¤„ç†<search>æ ‡ç­¾
        import re
        search_match = re.search(r'<search>(.*?)</search>', query, re.IGNORECASE | re.DOTALL)
        
        if search_match:
            # æå–æ ‡ç­¾å†…å®¹
            search_content = search_match.group(1).strip()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç®¡é“ç¬¦å·
            if '|' in search_content:
                # å¹¶è¡Œæœç´¢ï¼šåˆ†å‰²æŸ¥è¯¢
                search_tags = [q.strip() for q in search_content.split('|') if q.strip()]
            else:
                # å•ä¸€æœç´¢
                search_tags = [search_content]
        else:
            # æ²¡æœ‰<search>æ ‡ç­¾ï¼Œä½œä¸ºå•ä¸€æŸ¥è¯¢å¤„ç†
            search_tags = [query]
        
        if len(search_tags) > 1:
            # å¹¶è¡Œæœç´¢æ¨¡å¼
            logger.info(f"ğŸ” æ£€æµ‹åˆ°å¹¶è¡Œæœç´¢: {len(search_tags)} ä¸ªæŸ¥è¯¢")
            
            # KISSæ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨å·¥å…·å®ä¾‹ï¼Œsessionè‡ªåŠ¨ç®¡ç†
            # ä¸ºæ¯ä¸ªæŸ¥è¯¢åˆ†é…ç»“æœæ•°é‡
            results_per_query = max(1, num_results // len(search_tags))
            
            # æ ¹æ®æ¨¡å¼è®¾ç½®å¹¶å‘é™åˆ¶ï¼Œé¿å…åŒæ—¶å‘èµ·å¤ªå¤šè¯·æ±‚
            extractor_max_concurrent = getattr(tool_instance.content_extractor, "max_concurrent", 4)
            if mode == "light":
                # Lightæ¨¡å¼ï¼šä¼˜å…ˆé€Ÿåº¦ï¼Œæ”¯æŒæ›´é«˜å¹¶å‘
                max_concurrent = min(max(1, extractor_max_concurrent), len(search_tags))
                search_timeout = 60.0  # å•æŸ¥è¯¢60ç§’è¶…æ—¶
            else:
                # Normal/Deepæ¨¡å¼ï¼šæ›´ä¿å®ˆçš„å¹¶å‘ç­–ç•¥
                max_concurrent = min(max(1, extractor_max_concurrent), len(search_tags))
                search_timeout = 120.0  # å•æŸ¥è¯¢120ç§’è¶…æ—¶
            
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def search_with_limit(idx: int, query: str):
                async with semaphore:
                    logger.debug(f"[{idx}] å¼€å§‹æœç´¢: {query}")
                    try:
                        result = await asyncio.wait_for(
                            tool_instance.search_and_fetch(query, results_per_query, mode, search_options=kwargs),
                            timeout=search_timeout
                        )
                        logger.debug(f"[{idx}] æœç´¢å®Œæˆ: {query}")
                        return result
                    except asyncio.TimeoutError:
                        logger.warning(f"[{idx}] æœç´¢æŸ¥è¯¢è¶…æ—¶({search_timeout}ç§’): {query}")
                        logger.debug(f"[{idx}] è¶…æ—¶ç±»å‹: å¹¶è¡Œæœç´¢æŸ¥è¯¢çº§åˆ« | æ¨¡å¼: {mode}")
                        raise
                    except Exception as e:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹å±‚è¶…æ—¶ä¼ é€’ä¸Šæ¥çš„
                        if "è½®è¯¢è¶…æ—¶" in str(e):
                            logger.error(f"[{idx}] BrightDataè½®è¯¢è¶…æ—¶: {query} - {str(e)}")
                            logger.debug(f"[{idx}] è¶…æ—¶ç±»å‹: BrightDataè½®è¯¢çº§åˆ«")
                        elif "SERPæœç´¢è¶…æ—¶" in str(e):
                            logger.error(f"[{idx}] BrightDataæœç´¢è¶…æ—¶: {query} - {str(e)}")
                            logger.debug(f"[{idx}] è¶…æ—¶ç±»å‹: BrightData APIçº§åˆ«")
                        else:
                            logger.error(f"[{idx}] æœç´¢å¤±è´¥: {query} - {str(e)}")
                        raise
            
            # åˆ›å»ºå—æ§çš„æœç´¢ä»»åŠ¡
            search_tasks = []
            for i, search_query in enumerate(search_tags):
                clean_query = search_query.strip()
                if clean_query:
                    task = asyncio.create_task(search_with_limit(i, clean_query))
                    search_tasks.append((i, clean_query, task))
            
            # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆï¼ˆæ•è·æ‰€æœ‰å¼‚å¸¸åŒ…æ‹¬è¶…æ—¶ï¼‰
            search_results = await asyncio.gather(
                *[task for _, _, task in search_tasks],
                return_exceptions=True
            )
            
            # æ•´åˆå¹¶è¡Œæœç´¢ç»“æœ
            all_results = []
            parallel_stats = {
                'total_queries': len(search_tags),
                'successful_queries': 0,
                'total_results': 0,
                'total_fetched': 0,
                'total_fetch_success': 0,
                'total_pdf_count': 0,
                'query_details': []
            }
            
            for (i, search_query, _), result in zip(search_tasks, search_results):
                if isinstance(result, Exception):
                    logger.error(f"å¹¶è¡Œæœç´¢å¤±è´¥ [{i}] {search_query}: {str(result)}")
                    parallel_stats['query_details'].append({
                        'query': search_query,
                        'query_index': i,
                        'success': False,
                        'error': str(result)
                    })
                elif result.get('success'):
                    parallel_stats['successful_queries'] += 1
                    query_results = result.get('results', [])
                    
                    # ä¸ºç»“æœæ·»åŠ æŸ¥è¯¢ç´¢å¼•
                    for item in query_results:
                        item['search_query'] = search_query
                        item['search_index'] = i
                    
                    all_results.extend(query_results)
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    stats = result.get('statistics', {})
                    parallel_stats['total_results'] += stats.get('total_results', 0)
                    parallel_stats['total_fetched'] += stats.get('auto_fetched', 0)
                    parallel_stats['total_fetch_success'] += stats.get('fetch_success', 0)
                    parallel_stats['total_pdf_count'] += stats.get('pdf_count', 0)
                    
                    parallel_stats['query_details'].append({
                        'query': search_query,
                        'query_index': i,
                        'success': True,
                        'results_count': len(query_results),
                        'statistics': stats
                    })
                else:
                    logger.warning(f"å¹¶è¡Œæœç´¢å¤±è´¥ [{i}] {search_query}: {result.get('error', 'Unknown error')}")
                    parallel_stats['query_details'].append({
                        'query': search_query,
                        'query_index': i,
                        'success': False,
                        'error': result.get('error', 'Search failed')
                    })
            
            logger.info(f"âœ… å¹¶è¡Œæœç´¢å®Œæˆ: {parallel_stats['successful_queries']}/{parallel_stats['total_queries']} æˆåŠŸ")
            
            return {
                    'success': True,
                    'query': query,
                    'mode': mode,
                    'search_type': 'parallel',
                    'parallel_queries': search_tags,
                    'results': all_results,
                    'statistics': parallel_stats
                }
        else:
            # å•ä¸€æœç´¢æ¨¡å¼ - KISSæ–¹æ¡ˆï¼šä½¿ç”¨æå–çš„æŸ¥è¯¢å†…å®¹
            actual_query = search_tags[0] if search_tags else query
            result = await tool_instance.search_and_fetch(actual_query, num_results, mode, search_options=kwargs)
            # æ·»åŠ æœç´¢ç±»å‹æ ‡è¯†
            result['search_type'] = 'single'
            return result
    
    return StructuredTool.from_function(
        func=auto_search,
        coroutine=auto_search,
        name="auto_search",
        description=(
            "Advanced automated search tool with intelligent failure learning and single/parallel search capabilities: "
            "SINGLE SEARCH: Use <search>query</search> format, e.g., '<search>OpenAI GPT-4</search>' "
            "PARALLEL SEARCH: Use pipe symbol within search tags, e.g., '<search>Tesla stock|Apple stock|Microsoft stock</search>' "
            "MODES: 1) mode='light' - Returns search results with snippets only (fast) "
            "2) mode='full' - Automatically fetches full content and parses PDFs (comprehensive). "
            "SMART FEATURES: Learns from crawl failures, automatically skips known failing URLs, uses SERP snippets as fallback content for better efficiency. "
            "Parallel searches distribute num_results across all queries and aggregate results with query tracking."
        )
    )


# ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    """è‡ªåŠ¨æœç´¢å·¥å…·ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    async with AutoSearchTool(
        brightdata_api_key="your-brightdata-key",
        firecrawl_api_key="your-firecrawl-key"
    ) as tool:
        
        # æ‰§è¡Œæœç´¢ - è‡ªåŠ¨æŠ“å–å’Œå¤„ç†æ‰€æœ‰å†…å®¹
        result = await tool.search_and_fetch(
            query="RAG optimization techniques 2024",
            num_results=10
        )
        
        if result['success']:
            print(f"æœç´¢æˆåŠŸï¼ç»Ÿè®¡ä¿¡æ¯: {result['statistics']}")
            
            for item in result['results'][:3]:
                print(f"\næ ‡é¢˜: {item['title']}")
                print(f"URL: {item['url']}")
                
                if item.get('fetch_success'):
                    print(f"å†…å®¹é•¿åº¦: {item.get('content_length', 0)}")
                    print(f"æ˜¯å¦PDF: {item.get('is_pdf', False)}")
                    print(f"æå–æ–¹æ³•: {item.get('extraction_method', 'unknown')}")
                    
                    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                    content = item.get('content', '')
                    if content:
                        preview = content[:200] + '...' if len(content) > 200 else content
                        print(f"å†…å®¹é¢„è§ˆ: {preview}")
                else:
                    print(f"æŠ“å–å¤±è´¥: {item.get('fetch_error', 'Unknown')}")


if __name__ == "__main__":
    asyncio.run(example_usage())